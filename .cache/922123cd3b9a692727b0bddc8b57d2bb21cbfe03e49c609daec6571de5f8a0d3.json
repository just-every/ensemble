{
  "url": "https://ai.google.dev/gemini-api/docs/live",
  "markdown": "# Get started with Live API  |  Gemini API  |  Google AI for Developers\n[Skip to main content](#main-content)\n [![Google AI for Developers](https://www.gstatic.com/devrel-devsite/prod/va55008f56463f12ba1a0c4ec3fdc81dac4d4d331f95ef7b209d2570e7d9e879b/googledevai/images/lockup-new.svg)](https://ai.google.dev/)\n[Models](https://ai.google.dev/gemini-api/docs)\n-   Gemini\n-   [\n    About\n    ](https://deepmind.google/gemini)\n-   [\n    Docs\n    ](https://ai.google.dev/gemini-api/docs)\n-   [\n    API reference\n    ](https://ai.google.dev/api)\n-   [\n    Pricing\n    ](https://ai.google.dev/pricing)\n-   Imagen\n-   [\n    About\n    ](https://deepmind.google/technologies/imagen-3/)\n-   [\n    Docs\n    ](https://ai.google.dev/gemini-api/docs/image-generation#imagen)\n-   [\n    Pricing\n    ](https://ai.google.dev/pricing)\n-   Veo\n-   [\n    About\n    ](https://deepmind.google/technologies/veo/veo-2/)\n-   [\n    Docs\n    ](https://ai.google.dev/gemini-api/docs/video)\n-   [\n    Pricing\n    ](https://ai.google.dev/pricing)\n-   Gemma\n-   [\n    About\n    ](https://deepmind.google/models/gemma)\n-   [\n    Docs\n    ](https://ai.google.dev/gemma/docs)\n-   [\n    Gemmaverse\n    ](https://ai.google.dev/gemma/gemmaverse)\nSolutions\n-   Build with Gemini\n-   [\n    Gemini API\n    ](https://ai.google.dev/gemini-api/docs)\n-   [\n    Google AI Studio\n    ](https://aistudio.google.com)\n-   Customize Gemma open models\n-   [\n    Gemma open models\n    ](https://ai.google.dev/gemma)\n-   [\n    Multi-framework with Keras\n    ](https://keras.io/keras_3/)\n-   [\n    Fine-tune in Colab\n    ](https://colab.sandbox.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb)\n-   Run on-device\n-   [\n    Google AI Edge\n    ](https://ai.google.dev/edge)\n-   [\n    Gemini Nano on Android\n    ](https://developer.android.com/ai/gemini-nano)\n-   [\n    Chrome built-in web APIs\n    ](https://developer.chrome.com/docs/ai/built-in)\n-   Build responsibly\n-   [\n    Responsible GenAI Toolkit\n    ](https://ai.google.dev/responsible)\n-   [\n    Secure AI Framework\n    ](https://saif.google)\nCode assistance\n-   [\n    Android Studio\n    ](https://developer.android.com/gemini-in-android)\n-   [\n    Chrome DevTools\n    ](https://developer.chrome.com/docs/devtools/console/understand-messages)\n-   [\n    Colab\n    ](https://colab.google)\n-   [\n    Firebase\n    ](https://firebase.google.com/products/generative-ai)\n-   [\n    Google Cloud\n    ](https://cloud.google.com/products/gemini/code-assist)\n-   [\n    JetBrains\n    ](https://plugins.jetbrains.com/plugin/8079-google-cloud-code)\n-   [\n    Jules\n    ](https://labs.google.com/jules/home)\n-   [\n    VS Code\n    ](https://marketplace.visualstudio.com/items?itemName=GoogleCloudTools.cloudcode)\nShowcase\n-   [\n    Gemini Showcase\n    ](https://ai.google.dev/showcase)\n-   [\n    Gemini API Developer Competition\n    ](https://ai.google.dev/competition)\nCommunity\n-   [\n    Google AI Forum\n    ](https://discuss.ai.google.dev)\n-   [\n    Gemini for Research\n    ](https://ai.google.dev/gemini-api/docs/gemini-for-research)\n/\n-   English\n-   Deutsch\n-   Español – América Latina\n-   Français\n-   Indonesia\n-   Italiano\n-   Polski\n-   Português – Brasil\n-   Shqip\n-   Tiếng Việt\n-   Türkçe\n-   Русский\n-   עברית\n-   العربيّة\n-   فارسی\n-   हिंदी\n-   বাংলা\n-   ภาษาไทย\n-   中文 – 简体\n-   中文 – 繁體\n-   日本語\n-   한국어\nSign in\n[Gemini API docs](https://ai.google.dev/gemini-api/docs) [API Reference](https://ai.google.dev/api) [Cookbook](https://github.com/google-gemini/cookbook) [Community](https://discuss.ai.google.dev/c/gemini-api/) [![Google AI for Developers](https://www.gstatic.com/devrel-devsite/prod/va55008f56463f12ba1a0c4ec3fdc81dac4d4d331f95ef7b209d2570e7d9e879b/googledevai/images/lockup-new.svg)](https://ai.google.dev/)\n-   [Models](https://ai.google.dev/gemini-api/docs)\n    -   More\n    -   [Gemini API docs](https://ai.google.dev/gemini-api/docs)\n    -   [API Reference](https://ai.google.dev/api)\n    -   [Cookbook](https://github.com/google-gemini/cookbook)\n    -   [Community](https://discuss.ai.google.dev/c/gemini-api/)\n-   Solutions\n    -   More\n-   Code assistance\n    -   More\n-   Showcase\n    -   More\n-   Community\n    -   More\n-   Get started\n-   [Overview](https://ai.google.dev/gemini-api/docs)\n-   [Quickstart](https://ai.google.dev/gemini-api/docs/quickstart)\n-   [API keys](https://ai.google.dev/gemini-api/docs/api-key)\n-   [Libraries](https://ai.google.dev/gemini-api/docs/libraries)\n-   [OpenAI compatibility](https://ai.google.dev/gemini-api/docs/openai)\n-   Models\n-   [All models](https://ai.google.dev/gemini-api/docs/models)\n-   [Pricing](https://ai.google.dev/gemini-api/docs/pricing)\n-   [Rate limits](https://ai.google.dev/gemini-api/docs/rate-limits)\n-   [Billing info](https://ai.google.dev/gemini-api/docs/billing)\n-   Model Capabilities\n-   [Text generation](https://ai.google.dev/gemini-api/docs/text-generation)\n-   [Image generation](https://ai.google.dev/gemini-api/docs/image-generation)\n-   [Video generation](https://ai.google.dev/gemini-api/docs/video)\n-   [Speech generation](https://ai.google.dev/gemini-api/docs/speech-generation)\n-   [Music generation](https://ai.google.dev/gemini-api/docs/music-generation)\n-   [Long context](https://ai.google.dev/gemini-api/docs/long-context)\n-   [Structured output](https://ai.google.dev/gemini-api/docs/structured-output)\n-   [Thinking](https://ai.google.dev/gemini-api/docs/thinking)\n-   [Function calling](https://ai.google.dev/gemini-api/docs/function-calling)\n-   [Document understanding](https://ai.google.dev/gemini-api/docs/document-processing)\n-   [Image understanding](https://ai.google.dev/gemini-api/docs/image-understanding)\n-   [Video understanding](https://ai.google.dev/gemini-api/docs/video-understanding)\n-   [Audio understanding](https://ai.google.dev/gemini-api/docs/audio)\n-   [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)\n-   [URL context](https://ai.google.dev/gemini-api/docs/url-context)\n-   [Google Search](https://ai.google.dev/gemini-api/docs/google-search)\n-   Guides\n-   [Prompt engineering](https://ai.google.dev/gemini-api/docs/prompting-strategies)\n-   Live API\n    -   [Get started](https://ai.google.dev/gemini-api/docs/live)\n    -   [Capabilities](https://ai.google.dev/gemini-api/docs/live-guide)\n    -   [Tool use](https://ai.google.dev/gemini-api/docs/live-tools)\n    -   [Session management](https://ai.google.dev/gemini-api/docs/live-session)\n    -   [Ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens)\n-   [Context caching](https://ai.google.dev/gemini-api/docs/caching)\n-   [Files API](https://ai.google.dev/gemini-api/docs/files)\n-   [Token counting](https://ai.google.dev/gemini-api/docs/tokens)\n-   Fine-tuning\n    -   [Intro to fine-tuning](https://ai.google.dev/gemini-api/docs/model-tuning)\n    -   [Fine-tuning tutorial](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial)\n-   [Embeddings](https://ai.google.dev/gemini-api/docs/embeddings)\n-   Safety\n    -   [Safety settings](https://ai.google.dev/gemini-api/docs/safety-settings)\n    -   [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance)\n-   Resources\n-   [Migrate to Gen AI SDK](https://ai.google.dev/gemini-api/docs/migrate)\n-   [Release notes](https://ai.google.dev/gemini-api/docs/changelog)\n-   [API troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshooting)\n-   Open-Source Frameworks\n    -   [LangChain & LangGraph](https://ai.google.dev/gemini-api/docs/langgraph-example)\n    -   [CrewAI](https://ai.google.dev/gemini-api/docs/crewai-example)\n-   AI Studio\n    -   [Google AI Studio quickstart](https://ai.google.dev/gemini-api/docs/ai-studio-quickstart)\n    -   [LearnLM](https://ai.google.dev/gemini-api/docs/learnlm)\n    -   [AI Studio troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio)\n    -   [Google Workspace](https://ai.google.dev/gemini-api/docs/workspace)\n-   Google Cloud Platform\n    -   [VertexAI Gemini API](https://ai.google.dev/gemini-api/docs/migrate-to-cloud)\n    -   [OAuth authentication](https://ai.google.dev/gemini-api/docs/oauth)\n-   Policies\n-   [Terms of service](https://ai.google.dev/gemini-api/terms)\n-   [Available regions](https://ai.google.dev/gemini-api/docs/available-regions)\n-   [Additional usage polices](https://ai.google.dev/gemini-api/docs/usage-policies)\n-   Gemini\n-   [About](https://deepmind.google/gemini)\n-   [Docs](https://ai.google.dev/gemini-api/docs)\n-   [API reference](https://ai.google.dev/api)\n-   [Pricing](https://ai.google.dev/pricing)\n-   Imagen\n-   [About](https://deepmind.google/technologies/imagen-3/)\n-   [Docs](https://ai.google.dev/gemini-api/docs/image-generation#imagen)\n-   [Pricing](https://ai.google.dev/pricing)\n-   Veo\n-   [About](https://deepmind.google/technologies/veo/veo-2/)\n-   [Docs](https://ai.google.dev/gemini-api/docs/video)\n-   [Pricing](https://ai.google.dev/pricing)\n-   Gemma\n-   [About](https://deepmind.google/models/gemma)\n-   [Docs](https://ai.google.dev/gemma/docs)\n-   [Gemmaverse](https://ai.google.dev/gemma/gemmaverse)\n-   Build with Gemini\n-   [Gemini API](https://ai.google.dev/gemini-api/docs)\n-   [Google AI Studio](https://aistudio.google.com)\n-   Customize Gemma open models\n-   [Gemma open models](https://ai.google.dev/gemma)\n-   [Multi-framework with Keras](https://keras.io/keras_3/)\n-   [Fine-tune in Colab](https://colab.sandbox.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb)\n-   Run on-device\n-   [Google AI Edge](https://ai.google.dev/edge)\n-   [Gemini Nano on Android](https://developer.android.com/ai/gemini-nano)\n-   [Chrome built-in web APIs](https://developer.chrome.com/docs/ai/built-in)\n-   Build responsibly\n-   [Responsible GenAI Toolkit](https://ai.google.dev/responsible)\n-   [Secure AI Framework](https://saif.google)\n-   [Android Studio](https://developer.android.com/gemini-in-android)\n-   [Chrome DevTools](https://developer.chrome.com/docs/devtools/console/understand-messages)\n-   [Colab](https://colab.google)\n-   [Firebase](https://firebase.google.com/products/generative-ai)\n-   [Google Cloud](https://cloud.google.com/products/gemini/code-assist)\n-   [JetBrains](https://plugins.jetbrains.com/plugin/8079-google-cloud-code)\n-   [Jules](https://labs.google.com/jules/home)\n-   [VS Code](https://marketplace.visualstudio.com/items?itemName=GoogleCloudTools.cloudcode)\n-   [Gemini Showcase](https://ai.google.dev/showcase)\n-   [Gemini API Developer Competition](https://ai.google.dev/competition)\n-   [Google AI Forum](https://discuss.ai.google.dev)\n-   [Gemini for Research](https://ai.google.dev/gemini-api/docs/gemini-for-research)\nIntroducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)\n-   [Home](https://ai.google.dev/)\n-   [Gemini API](https://ai.google.dev/gemini-api)\n-   [Models](https://ai.google.dev/gemini-api/docs)\nSend feedback\n# Get started with Live API\n**Preview:** The Live API is in preview.\nThe Live API enables low-latency, real-time voice and video interactions with Gemini. It processes continuous streams of audio, video, or text to deliver immediate, human-like spoken responses, creating a natural conversational experience for your users.\n![Live API Overview](https://ai.google.dev/static/gemini-api/docs/images/live-api-overview.png)\nLive API offers a comprehensive set of features such as [Voice Activity Detection](https://ai.google.dev/gemini-api/docs/live-guide#interruptions), [tool use and function calling](https://ai.google.dev/gemini-api/docs/live-tools), [session management](https://ai.google.dev/gemini-api/docs/live-session) (for managing long running conversations) and [ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens) (for secure client-sided authentication).\nThis page gets you up and running with examples and basic code samples.\n## Example applications\nCheck out the following example applications that illustrate how to use Live API for end-to-end use cases:\n-   [Live audio starter app](https://aistudio.google.com/apps/bundled/live_audio?showPreview=true&showCode=true&showAssistant=false) on AI Studio, using JavaScript libraries to connect to Live API and stream bidirectional audio through your microphone and speakers.\n-   Live API [Python cookbook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py) using Pyaudio that connects to Live API.\n## Partner integrations\nIf you prefer a simpler development process, you can use [Daily](https://www.daily.co/products/gemini/multimodal-live-api/) or [LiveKit](https://docs.livekit.io/agents/integrations/google/#multimodal-live-api). These are third-party partner platforms that have already integrated the Gemini Live API over the WebRTC protocol to streamline the development of real-time audio and video applications.\n## Before you begin building\nThere are two important decisions to make before you begin building with the Live API: choosing a model and choosing an implementation approach.\n### Choose a model\nIf you're building an audio-based use case, your choice of model determines the audio generation architecture used to create the audio response:\n-   **[Native audio](https://ai.google.dev/gemini-api/docs/live-guide#native-audio-output) with [Gemini 2.5 Flash](https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-native-audio):** This option provides the most natural and realistic-sounding speech and better multilingual performance. It also enables advanced features like [affective (emotion-aware) dialogue](https://ai.google.dev/gemini-api/docs/live-guide#affective-dialog), [proactive audio](https://ai.google.dev/gemini-api/docs/live-guide#proactive-audio) (where the model can decide to ignore or respond to certain inputs), and [\"thinking\"](https://ai.google.dev/gemini-api/docs/live-guide#native-audio-output-thinking). Native audio is supported by the following [native audio models](https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-native-audio):\n    -   `gemini-2.5-flash-preview-native-audio-dialog`\n    -   `gemini-2.5-flash-exp-native-audio-thinking-dialog`\n-   **Half-cascade audio with [Gemini 2.0 Flash](https://ai.google.dev/gemini-api/docs/models#live-api)**: This option, available with the `gemini-2.0-flash-live-001` model, uses a cascaded model architecture (native audio input and text-to-speech output). It offers better performance and reliability in production environments, especially with [tool use](https://ai.google.dev/gemini-api/doc/live-tools).\n### Choose an implementation approach\nWhen integrating with Live API, you'll need to choose one of the following implementation approaches:\n-   **Server-to-server**: Your backend connects to the Live API using [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API). Typically, your client sends stream data (audio, video, text) to your server, which then forwards it to the Live API.\n-   **Client-to-server**: Your frontend code connects directly to the Live API using [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) to stream data, bypassing your backend.\n**Note:** Client-to-server generally offers better performance for streaming audio and video, since it bypasses the need to send the stream to your backend first. It's also easier to set up since you don't need to implement a proxy that sends data from your client to your server and then your server to the API. However, for production environments, in order to mitigate security risks, we recommend using [ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens) instead of standard API keys.\n## Get started\nThis example **_reads a WAV file_**, sends it in the correct format, and saves the received data as WAV file.\nYou can send audio by converting it to 16-bit PCM, 16kHz, mono format, and you can receive audio by setting `AUDIO` as response modality. The output uses a sample rate of 24kHz.\n### Python\n```\n# Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav\n# Install helpers for converting files: pip install librosa soundfile\nimport asyncio\nimport io\nfrom pathlib import Path\nimport wave\nfrom google import genai\nfrom google.genai import types\nimport soundfile as sf\nimport librosa\nclient = genai.Client(api_key=\"GEMINI_API_KEY\")\n# Half cascade model:\n# model = \"gemini-2.0-flash-live-001\"\n# Native audio output model:\nmodel = \"gemini-2.5-flash-preview-native-audio-dialog\"\nconfig = {\n  \"response_modalities\": [\"AUDIO\"],\n  \"system_instruction\": \"You are a helpful assistant and answer in a friendly tone.\",\n}\nasync def main():\n    async with client.aio.live.connect(model=model, config=config) as session:\n        buffer = io.BytesIO()\n        y, sr = librosa.load(\"sample.wav\", sr=16000)\n        sf.write(buffer, y, sr, format='RAW', subtype='PCM_16')\n        buffer.seek(0)\n        audio_bytes = buffer.read()\n        # If already in correct format, you can use this:\n        # audio_bytes = Path(\"sample.pcm\").read_bytes()\n        await session.send_realtime_input(\n            audio=types.Blob(data=audio_bytes, mime_type=\"audio/pcm;rate=16000\")\n        )\n        wf = wave.open(\"audio.wav\", \"wb\")\n        wf.setnchannels(1)\n        wf.setsampwidth(2)\n        wf.setframerate(24000)  # Output is 24kHz\n        async for response in session.receive():\n            if response.data is not None:\n                wf.writeframes(response.data)\n            # Un-comment this code to print audio data info\n            # if response.server_content.model_turn is not None:\n            #      print(response.server_content.model_turn.parts[0].inline_data.mime_type)\n        wf.close()\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n### JavaScript\n```\n// Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav\nimport { GoogleGenAI, Modality } from '@google/genai';\nimport * as fs from \"node:fs\";\nimport pkg from 'wavefile';  // npm install wavefile\nconst { WaveFile } = pkg;\nconst ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n// WARNING: Do not use API keys in client-side (browser based) applications\n// Consider using Ephemeral Tokens instead\n// More information at: https://ai.google.dev/gemini-api/docs/ephemeral-tokens\n// Half cascade model:\n// const model = \"gemini-2.0-flash-live-001\"\n// Native audio output model:\nconst model = \"gemini-2.5-flash-preview-native-audio-dialog\"\nconst config = {\n  responseModalities: [Modality.AUDIO],\n  systemInstruction: \"You are a helpful assistant and answer in a friendly tone.\"\n};\nasync function live() {\n    const responseQueue = [];\n    async function waitMessage() {\n        let done = false;\n        let message = undefined;\n        while (!done) {\n            message = responseQueue.shift();\n            if (message) {\n                done = true;\n            } else {\n                await new Promise((resolve) => setTimeout(resolve, 100));\n            }\n        }\n        return message;\n    }\n    async function handleTurn() {\n        const turns = [];\n        let done = false;\n        while (!done) {\n            const message = await waitMessage();\n            turns.push(message);\n            if (message.serverContent && message.serverContent.turnComplete) {\n                done = true;\n            }\n        }\n        return turns;\n    }\n    const session = await ai.live.connect({\n        model: model,\n        callbacks: {\n            onopen: function () {\n                console.debug('Opened');\n            },\n            onmessage: function (message) {\n                responseQueue.push(message);\n            },\n            onerror: function (e) {\n                console.debug('Error:', e.message);\n            },\n            onclose: function (e) {\n                console.debug('Close:', e.reason);\n            },\n        },\n        config: config,\n    });\n    // Send Audio Chunk\n    const fileBuffer = fs.readFileSync(\"sample.wav\");\n    // Ensure audio conforms to API requirements (16-bit PCM, 16kHz, mono)\n    const wav = new WaveFile();\n    wav.fromBuffer(fileBuffer);\n    wav.toSampleRate(16000);\n    wav.toBitDepth(\"16\");\n    const base64Audio = wav.toBase64();\n    // If already in correct format, you can use this:\n    // const fileBuffer = fs.readFileSync(\"sample.pcm\");\n    // const base64Audio = Buffer.from(fileBuffer).toString('base64');\n    session.sendRealtimeInput(\n        {\n            audio: {\n                data: base64Audio,\n                mimeType: \"audio/pcm;rate=16000\"\n            }\n        }\n    );\n    const turns = await handleTurn();\n    // Combine audio data strings and save as wave file\n    const combinedAudio = turns.reduce((acc, turn) => {\n        if (turn.data) {\n            const buffer = Buffer.from(turn.data, 'base64');\n            const intArray = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / Int16Array.BYTES_PER_ELEMENT);\n            return acc.concat(Array.from(intArray));\n        }\n        return acc;\n    }, []);\n    const audioBuffer = new Int16Array(combinedAudio);\n    const wf = new WaveFile();\n    wf.fromScratch(1, 24000, '16', audioBuffer);  // output is 24kHz\n    fs.writeFileSync('audio.wav', wf.toBuffer());\n    session.close();\n}\nasync function main() {\n    await live().catch((e) => console.error('got error', e));\n}\nmain();\n```\n## What's next\n-   Read the full Live API [Capabilities](https://ai.google.dev/gemini-api/docs/live-guide) guide for key capabilities and configurations; including Voice Activity Detection and native audio features.\n-   Read the [Tool use](https://ai.google.dev/gemini-api/docs/live-tools) guide to learn how to integrate Live API with tools and function calling.\n-   Read the [Session management](https://ai.google.dev/gemini-api/docs/live-session) guide for managing long running conversations.\n-   Read the [Ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens) guide for secure authentication in [client-to-server](#implementation-approach) applications.\n-   For more information about the underlying WebSockets API, see the [WebSockets API reference](https://ai.google.dev/api/live).\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-06-17 UTC.\n-   [Terms](//policies.google.com/terms)\n-   [Privacy](//policies.google.com/privacy)\n-   [Manage cookies](#)\n-   English\n-   Deutsch\n-   Español – América Latina\n-   Français\n-   Indonesia\n-   Italiano\n-   Polski\n-   Português – Brasil\n-   Shqip\n-   Tiếng Việt\n-   Türkçe\n-   Русский\n-   עברית\n-   العربيّة\n-   فارسی\n-   हिंदी\n-   বাংলা\n-   ภาษาไทย\n-   中文 – 简体\n-   中文 – 繁體\n-   日本語\n-   한국어",
  "timestamp": 1750652720279,
  "title": "Get started with Live API  |  Gemini API  |  Google AI for Developers"
}