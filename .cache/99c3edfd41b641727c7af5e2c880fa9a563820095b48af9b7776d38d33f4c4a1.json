{
  "url": "https://ai.google.dev/gemini-api/docs/live-tools",
  "markdown": "# Tool use with Live API  |  Gemini API  |  Google AI for Developers\n[Skip to main content](#main-content)\n [![Google AI for Developers](https://www.gstatic.com/devrel-devsite/prod/va55008f56463f12ba1a0c4ec3fdc81dac4d4d331f95ef7b209d2570e7d9e879b/googledevai/images/lockup-new.svg)](https://ai.google.dev/)\n[Models](https://ai.google.dev/gemini-api/docs)\n-   Gemini\n-   [\n    About\n    ](https://deepmind.google/gemini)\n-   [\n    Docs\n    ](https://ai.google.dev/gemini-api/docs)\n-   [\n    API reference\n    ](https://ai.google.dev/api)\n-   [\n    Pricing\n    ](https://ai.google.dev/pricing)\n-   Imagen\n-   [\n    About\n    ](https://deepmind.google/technologies/imagen-3/)\n-   [\n    Docs\n    ](https://ai.google.dev/gemini-api/docs/image-generation#imagen)\n-   [\n    Pricing\n    ](https://ai.google.dev/pricing)\n-   Veo\n-   [\n    About\n    ](https://deepmind.google/technologies/veo/veo-2/)\n-   [\n    Docs\n    ](https://ai.google.dev/gemini-api/docs/video)\n-   [\n    Pricing\n    ](https://ai.google.dev/pricing)\n-   Gemma\n-   [\n    About\n    ](https://deepmind.google/models/gemma)\n-   [\n    Docs\n    ](https://ai.google.dev/gemma/docs)\n-   [\n    Gemmaverse\n    ](https://ai.google.dev/gemma/gemmaverse)\nSolutions\n-   Build with Gemini\n-   [\n    Gemini API\n    ](https://ai.google.dev/gemini-api/docs)\n-   [\n    Google AI Studio\n    ](https://aistudio.google.com)\n-   Customize Gemma open models\n-   [\n    Gemma open models\n    ](https://ai.google.dev/gemma)\n-   [\n    Multi-framework with Keras\n    ](https://keras.io/keras_3/)\n-   [\n    Fine-tune in Colab\n    ](https://colab.sandbox.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb)\n-   Run on-device\n-   [\n    Google AI Edge\n    ](https://ai.google.dev/edge)\n-   [\n    Gemini Nano on Android\n    ](https://developer.android.com/ai/gemini-nano)\n-   [\n    Chrome built-in web APIs\n    ](https://developer.chrome.com/docs/ai/built-in)\n-   Build responsibly\n-   [\n    Responsible GenAI Toolkit\n    ](https://ai.google.dev/responsible)\n-   [\n    Secure AI Framework\n    ](https://saif.google)\nCode assistance\n-   [\n    Android Studio\n    ](https://developer.android.com/gemini-in-android)\n-   [\n    Chrome DevTools\n    ](https://developer.chrome.com/docs/devtools/console/understand-messages)\n-   [\n    Colab\n    ](https://colab.google)\n-   [\n    Firebase\n    ](https://firebase.google.com/products/generative-ai)\n-   [\n    Google Cloud\n    ](https://cloud.google.com/products/gemini/code-assist)\n-   [\n    JetBrains\n    ](https://plugins.jetbrains.com/plugin/8079-google-cloud-code)\n-   [\n    Jules\n    ](https://labs.google.com/jules/home)\n-   [\n    VS Code\n    ](https://marketplace.visualstudio.com/items?itemName=GoogleCloudTools.cloudcode)\nShowcase\n-   [\n    Gemini Showcase\n    ](https://ai.google.dev/showcase)\n-   [\n    Gemini API Developer Competition\n    ](https://ai.google.dev/competition)\nCommunity\n-   [\n    Google AI Forum\n    ](https://discuss.ai.google.dev)\n-   [\n    Gemini for Research\n    ](https://ai.google.dev/gemini-api/docs/gemini-for-research)\n/\n-   English\n-   Deutsch\n-   Español – América Latina\n-   Français\n-   Indonesia\n-   Italiano\n-   Polski\n-   Português – Brasil\n-   Shqip\n-   Tiếng Việt\n-   Türkçe\n-   Русский\n-   עברית\n-   العربيّة\n-   فارسی\n-   हिंदी\n-   বাংলা\n-   ภาษาไทย\n-   中文 – 简体\n-   中文 – 繁體\n-   日本語\n-   한국어\nSign in\n[Gemini API docs](https://ai.google.dev/gemini-api/docs) [API Reference](https://ai.google.dev/api) [Cookbook](https://github.com/google-gemini/cookbook) [Community](https://discuss.ai.google.dev/c/gemini-api/) [![Google AI for Developers](https://www.gstatic.com/devrel-devsite/prod/va55008f56463f12ba1a0c4ec3fdc81dac4d4d331f95ef7b209d2570e7d9e879b/googledevai/images/lockup-new.svg)](https://ai.google.dev/)\n-   [Models](https://ai.google.dev/gemini-api/docs)\n    -   More\n    -   [Gemini API docs](https://ai.google.dev/gemini-api/docs)\n    -   [API Reference](https://ai.google.dev/api)\n    -   [Cookbook](https://github.com/google-gemini/cookbook)\n    -   [Community](https://discuss.ai.google.dev/c/gemini-api/)\n-   Solutions\n    -   More\n-   Code assistance\n    -   More\n-   Showcase\n    -   More\n-   Community\n    -   More\n-   Get started\n-   [Overview](https://ai.google.dev/gemini-api/docs)\n-   [Quickstart](https://ai.google.dev/gemini-api/docs/quickstart)\n-   [API keys](https://ai.google.dev/gemini-api/docs/api-key)\n-   [Libraries](https://ai.google.dev/gemini-api/docs/libraries)\n-   [OpenAI compatibility](https://ai.google.dev/gemini-api/docs/openai)\n-   Models\n-   [All models](https://ai.google.dev/gemini-api/docs/models)\n-   [Pricing](https://ai.google.dev/gemini-api/docs/pricing)\n-   [Rate limits](https://ai.google.dev/gemini-api/docs/rate-limits)\n-   [Billing info](https://ai.google.dev/gemini-api/docs/billing)\n-   Model Capabilities\n-   [Text generation](https://ai.google.dev/gemini-api/docs/text-generation)\n-   [Image generation](https://ai.google.dev/gemini-api/docs/image-generation)\n-   [Video generation](https://ai.google.dev/gemini-api/docs/video)\n-   [Speech generation](https://ai.google.dev/gemini-api/docs/speech-generation)\n-   [Music generation](https://ai.google.dev/gemini-api/docs/music-generation)\n-   [Long context](https://ai.google.dev/gemini-api/docs/long-context)\n-   [Structured output](https://ai.google.dev/gemini-api/docs/structured-output)\n-   [Thinking](https://ai.google.dev/gemini-api/docs/thinking)\n-   [Function calling](https://ai.google.dev/gemini-api/docs/function-calling)\n-   [Document understanding](https://ai.google.dev/gemini-api/docs/document-processing)\n-   [Image understanding](https://ai.google.dev/gemini-api/docs/image-understanding)\n-   [Video understanding](https://ai.google.dev/gemini-api/docs/video-understanding)\n-   [Audio understanding](https://ai.google.dev/gemini-api/docs/audio)\n-   [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)\n-   [URL context](https://ai.google.dev/gemini-api/docs/url-context)\n-   [Google Search](https://ai.google.dev/gemini-api/docs/google-search)\n-   Guides\n-   [Prompt engineering](https://ai.google.dev/gemini-api/docs/prompting-strategies)\n-   Live API\n    -   [Get started](https://ai.google.dev/gemini-api/docs/live)\n    -   [Capabilities](https://ai.google.dev/gemini-api/docs/live-guide)\n    -   [Tool use](https://ai.google.dev/gemini-api/docs/live-tools)\n    -   [Session management](https://ai.google.dev/gemini-api/docs/live-session)\n    -   [Ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens)\n-   [Context caching](https://ai.google.dev/gemini-api/docs/caching)\n-   [Files API](https://ai.google.dev/gemini-api/docs/files)\n-   [Token counting](https://ai.google.dev/gemini-api/docs/tokens)\n-   Fine-tuning\n    -   [Intro to fine-tuning](https://ai.google.dev/gemini-api/docs/model-tuning)\n    -   [Fine-tuning tutorial](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial)\n-   [Embeddings](https://ai.google.dev/gemini-api/docs/embeddings)\n-   Safety\n    -   [Safety settings](https://ai.google.dev/gemini-api/docs/safety-settings)\n    -   [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance)\n-   Resources\n-   [Migrate to Gen AI SDK](https://ai.google.dev/gemini-api/docs/migrate)\n-   [Release notes](https://ai.google.dev/gemini-api/docs/changelog)\n-   [API troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshooting)\n-   Open-Source Frameworks\n    -   [LangChain & LangGraph](https://ai.google.dev/gemini-api/docs/langgraph-example)\n    -   [CrewAI](https://ai.google.dev/gemini-api/docs/crewai-example)\n-   AI Studio\n    -   [Google AI Studio quickstart](https://ai.google.dev/gemini-api/docs/ai-studio-quickstart)\n    -   [LearnLM](https://ai.google.dev/gemini-api/docs/learnlm)\n    -   [AI Studio troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio)\n    -   [Google Workspace](https://ai.google.dev/gemini-api/docs/workspace)\n-   Google Cloud Platform\n    -   [VertexAI Gemini API](https://ai.google.dev/gemini-api/docs/migrate-to-cloud)\n    -   [OAuth authentication](https://ai.google.dev/gemini-api/docs/oauth)\n-   Policies\n-   [Terms of service](https://ai.google.dev/gemini-api/terms)\n-   [Available regions](https://ai.google.dev/gemini-api/docs/available-regions)\n-   [Additional usage polices](https://ai.google.dev/gemini-api/docs/usage-policies)\n-   Gemini\n-   [About](https://deepmind.google/gemini)\n-   [Docs](https://ai.google.dev/gemini-api/docs)\n-   [API reference](https://ai.google.dev/api)\n-   [Pricing](https://ai.google.dev/pricing)\n-   Imagen\n-   [About](https://deepmind.google/technologies/imagen-3/)\n-   [Docs](https://ai.google.dev/gemini-api/docs/image-generation#imagen)\n-   [Pricing](https://ai.google.dev/pricing)\n-   Veo\n-   [About](https://deepmind.google/technologies/veo/veo-2/)\n-   [Docs](https://ai.google.dev/gemini-api/docs/video)\n-   [Pricing](https://ai.google.dev/pricing)\n-   Gemma\n-   [About](https://deepmind.google/models/gemma)\n-   [Docs](https://ai.google.dev/gemma/docs)\n-   [Gemmaverse](https://ai.google.dev/gemma/gemmaverse)\n-   Build with Gemini\n-   [Gemini API](https://ai.google.dev/gemini-api/docs)\n-   [Google AI Studio](https://aistudio.google.com)\n-   Customize Gemma open models\n-   [Gemma open models](https://ai.google.dev/gemma)\n-   [Multi-framework with Keras](https://keras.io/keras_3/)\n-   [Fine-tune in Colab](https://colab.sandbox.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb)\n-   Run on-device\n-   [Google AI Edge](https://ai.google.dev/edge)\n-   [Gemini Nano on Android](https://developer.android.com/ai/gemini-nano)\n-   [Chrome built-in web APIs](https://developer.chrome.com/docs/ai/built-in)\n-   Build responsibly\n-   [Responsible GenAI Toolkit](https://ai.google.dev/responsible)\n-   [Secure AI Framework](https://saif.google)\n-   [Android Studio](https://developer.android.com/gemini-in-android)\n-   [Chrome DevTools](https://developer.chrome.com/docs/devtools/console/understand-messages)\n-   [Colab](https://colab.google)\n-   [Firebase](https://firebase.google.com/products/generative-ai)\n-   [Google Cloud](https://cloud.google.com/products/gemini/code-assist)\n-   [JetBrains](https://plugins.jetbrains.com/plugin/8079-google-cloud-code)\n-   [Jules](https://labs.google.com/jules/home)\n-   [VS Code](https://marketplace.visualstudio.com/items?itemName=GoogleCloudTools.cloudcode)\n-   [Gemini Showcase](https://ai.google.dev/showcase)\n-   [Gemini API Developer Competition](https://ai.google.dev/competition)\n-   [Google AI Forum](https://discuss.ai.google.dev)\n-   [Gemini for Research](https://ai.google.dev/gemini-api/docs/gemini-for-research)\nIntroducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)\n-   [Home](https://ai.google.dev/)\n-   [Gemini API](https://ai.google.dev/gemini-api)\n-   [Models](https://ai.google.dev/gemini-api/docs)\nSend feedback\n# Tool use with Live API\nTool use allows Live API to go beyond just conversation by enabling it to perform actions in the real-world and pull in external context while maintaining a real time connection. You can define tools such as [Function calling](https://ai.google.dev/gemini-api/docs/function-calling), [Code execution](https://ai.google.dev/gemini-api/docs/code-execution), and [Google Search](https://ai.google.dev/gemini-api/docs/grounding) with the Live API.\n## Overview of supported tools\nHere's a brief overview of the available tools for each model:\n| Tool | Cascaded models\n`gemini-live-2.5-flash-preview`\n`gemini-2.0-flash-live-001` | `gemini-2.5-flash-preview-native-audio-dialog` | `gemini-2.5-flash-exp-native-audio-thinking-dialog` |\n| --- | --- | --- | --- |\n| **Search** | Yes | Yes | Yes |\n| **Function calling** | Yes | Yes | No |\n| **Code execution** | Yes | No | No |\n| **Url context** | Yes | No | No |\n## Function calling\nLive API supports function calling, just like regular content generation requests. Function calling lets the Live API interact with external data and programs, greatly increasing what your applications can accomplish.\nYou can define function declarations as part of the session configuration. After receiving tool calls, the client should respond with a list of `FunctionResponse` objects using the `session.send_tool_response` method.\nSee the [Function calling tutorial](https://ai.google.dev/gemini-api/docs/function-calling) to learn more.\n**Note:** Unlike the `generateContent` API, the Live API doesn't support automatic tool response handling. You must handle tool responses manually in your client code.\n### Python\n```\nimport asyncio\nfrom google import genai\nfrom google.genai import types\nclient = genai.Client(api_key=\"GEMINI_API_KEY\")\nmodel = \"gemini-live-2.5-flash-preview\"\n# Simple function definitions\nturn_on_the_lights = {\"name\": \"turn_on_the_lights\"}\nturn_off_the_lights = {\"name\": \"turn_off_the_lights\"}\ntools = [{\"function_declarations\": [turn_on_the_lights, turn_off_the_lights]}]\nconfig = {\"response_modalities\": [\"TEXT\"], \"tools\": tools}\nasync def main():\n    async with client.aio.live.connect(model=model, config=config) as session:\n        prompt = \"Turn on the lights please\"\n        await session.send_client_content(turns={\"parts\": [{\"text\": prompt}]})\n        async for chunk in session.receive():\n            if chunk.server_content:\n                if chunk.text is not None:\n                    print(chunk.text)\n            elif chunk.tool_call:\n                function_responses = []\n                for fc in chunk.tool_call.function_calls:\n                    function_response = types.FunctionResponse(\n                        id=fc.id,\n                        name=fc.name,\n                        response={ \"result\": \"ok\" } # simple, hard-coded function response\n                    )\n                    function_responses.append(function_response)\n                await session.send_tool_response(function_responses=function_responses)\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n### JavaScript\n```\nimport { GoogleGenAI, Modality } from '@google/genai';\nconst ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\nconst model = 'gemini-live-2.5-flash-preview';\n// Simple function definitions\nconst turn_on_the_lights = { name: \"turn_on_the_lights\" } // , description: '...', parameters: { ... }\nconst turn_off_the_lights = { name: \"turn_off_the_lights\" }\nconst tools = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }]\nconst config = {\n  responseModalities: [Modality.TEXT],\n  tools: tools\n}\nasync function live() {\n  const responseQueue = [];\n  async function waitMessage() {\n    let done = false;\n    let message = undefined;\n    while (!done) {\n      message = responseQueue.shift();\n      if (message) {\n        done = true;\n      } else {\n        await new Promise((resolve) => setTimeout(resolve, 100));\n      }\n    }\n    return message;\n  }\n  async function handleTurn() {\n    const turns = [];\n    let done = false;\n    while (!done) {\n      const message = await waitMessage();\n      turns.push(message);\n      if (message.serverContent && message.serverContent.turnComplete) {\n        done = true;\n      } else if (message.toolCall) {\n        done = true;\n      }\n    }\n    return turns;\n  }\n  const session = await ai.live.connect({\n    model: model,\n    callbacks: {\n      onopen: function () {\n        console.debug('Opened');\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n      },\n      onerror: function (e) {\n        console.debug('Error:', e.message);\n      },\n      onclose: function (e) {\n        console.debug('Close:', e.reason);\n      },\n    },\n    config: config,\n  });\n  const inputTurns = 'Turn on the lights please';\n  session.sendClientContent({ turns: inputTurns });\n  let turns = await handleTurn();\n  for (const turn of turns) {\n    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {\n      for (const part of turn.serverContent.modelTurn.parts) {\n        if (part.text) {\n          console.debug('Received text: %s\\n', part.text);\n        }\n      }\n    }\n    else if (turn.toolCall) {\n      const functionResponses = [];\n      for (const fc of turn.toolCall.functionCalls) {\n        functionResponses.push({\n          id: fc.id,\n          name: fc.name,\n          response: { result: \"ok\" } // simple, hard-coded function response\n        });\n      }\n      console.debug('Sending tool response...\\n');\n      session.sendToolResponse({ functionResponses: functionResponses });\n    }\n  }\n  // Check again for new messages\n  turns = await handleTurn();\n  for (const turn of turns) {\n    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {\n      for (const part of turn.serverContent.modelTurn.parts) {\n        if (part.text) {\n          console.debug('Received text: %s\\n', part.text);\n        }\n      }\n    }\n  }\n  session.close();\n}\nasync function main() {\n  await live().catch((e) => console.error('got error', e));\n}\nmain();\n```\nFrom a single prompt, the model can generate multiple function calls and the code necessary to chain their outputs. This code executes in a sandbox environment, generating subsequent [BidiGenerateContentToolCall](https://ai.google.dev/api/live#bidigeneratecontenttoolcall) messages.\n## Asynchronous function calling\n**Note:** Asynchronous function calling is only supported in [half-cascade](https://ai.google.dev/gemini-api/docs/live#audio-generation) audio generation.\nFunction calling executes sequentially by default, meaning execution pauses until the results of each function call are available. This ensures sequential processing, which means you won't be able to continue interacting with the model while the functions are being run.\nIf you don't want to block the conversation, you can tell the model to run the functions asynchronously. To do so, you first need to add a `behavior` to the function definitions:\n### Python\n  ```\n  # Non-blocking function definitions\n  turn_on_the_lights = {\"name\": \"turn_on_the_lights\", \"behavior\": \"NON_BLOCKING\"} # turn_on_the_lights will run asynchronously\n  turn_off_the_lights = {\"name\": \"turn_off_the_lights\"} # turn_off_the_lights will still pause all interactions with the model\n```\n### JavaScript\n```\nimport { GoogleGenAI, Modality, Behavior } from '@google/genai';\n// Non-blocking function definitions\nconst turn_on_the_lights = {name: \"turn_on_the_lights\", behavior: Behavior.NON_BLOCKING}\n// Blocking function definitions\nconst turn_off_the_lights = {name: \"turn_off_the_lights\"}\nconst tools = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }]\n```\n`NON-BLOCKING` ensures the function runs asynchronously while you can continue interacting with the model.\nThen you need to tell the model how to behave when it receives the `FunctionResponse` using the `scheduling` parameter. It can either:\n-   Interrupt what it's doing and tell you about the response it got right away (`scheduling=\"INTERRUPT\"`),\n-   Wait until it's finished with what it's currently doing (`scheduling=\"WHEN_IDLE\"`),\n-   Or do nothing and use that knowledge later on in the discussion (`scheduling=\"SILENT\"`)\n### Python\n```\n# for a non-blocking function definition, apply scheduling in the function response:\n  function_response = types.FunctionResponse(\n      id=fc.id,\n      name=fc.name,\n      response={\n          \"result\": \"ok\",\n          \"scheduling\": \"INTERRUPT\" # Can also be WHEN_IDLE or SILENT\n      }\n  )\n```\n### JavaScript\n```\nimport { GoogleGenAI, Modality, Behavior, FunctionResponseScheduling } from '@google/genai';\n// for a non-blocking function definition, apply scheduling in the function response:\nconst functionResponse = {\n  id: fc.id,\n  name: fc.name,\n  response: {\n    result: \"ok\",\n    scheduling: FunctionResponseScheduling.INTERRUPT  // Can also be WHEN_IDLE or SILENT\n  }\n}\n```\n## Code execution\nYou can define code execution as part of the session configuration. This lets the Live API generate and execute Python code and dynamically perform computations to benefit your results. See the [Code execution tutorial](https://ai.google.dev/gemini-api/docs/code-execution) to learn more.\n### Python\n```\nimport asyncio\nfrom google import genai\nfrom google.genai import types\nclient = genai.Client(api_key=\"GEMINI_API_KEY\")\nmodel = \"gemini-live-2.5-flash-preview\"\ntools = [{'code_execution': {}}]\nconfig = {\"response_modalities\": [\"TEXT\"], \"tools\": tools}\nasync def main():\n    async with client.aio.live.connect(model=model, config=config) as session:\n        prompt = \"Compute the largest prime palindrome under 100000.\"\n        await session.send_client_content(turns={\"parts\": [{\"text\": prompt}]})\n        async for chunk in session.receive():\n            if chunk.server_content:\n                if chunk.text is not None:\n                    print(chunk.text)\n                model_turn = chunk.server_content.model_turn\n                if model_turn:\n                    for part in model_turn.parts:\n                      if part.executable_code is not None:\n                        print(part.executable_code.code)\n                      if part.code_execution_result is not None:\n                        print(part.code_execution_result.output)\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n### JavaScript\n```\nimport { GoogleGenAI, Modality } from '@google/genai';\nconst ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\nconst model = 'gemini-live-2.5-flash-preview';\nconst tools = [{codeExecution: {}}]\nconst config = {\n  responseModalities: [Modality.TEXT],\n  tools: tools\n}\nasync function live() {\n  const responseQueue = [];\n  async function waitMessage() {\n    let done = false;\n    let message = undefined;\n    while (!done) {\n      message = responseQueue.shift();\n      if (message) {\n        done = true;\n      } else {\n        await new Promise((resolve) => setTimeout(resolve, 100));\n      }\n    }\n    return message;\n  }\n  async function handleTurn() {\n    const turns = [];\n    let done = false;\n    while (!done) {\n      const message = await waitMessage();\n      turns.push(message);\n      if (message.serverContent && message.serverContent.turnComplete) {\n        done = true;\n      } else if (message.toolCall) {\n        done = true;\n      }\n    }\n    return turns;\n  }\n  const session = await ai.live.connect({\n    model: model,\n    callbacks: {\n      onopen: function () {\n        console.debug('Opened');\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n      },\n      onerror: function (e) {\n        console.debug('Error:', e.message);\n      },\n      onclose: function (e) {\n        console.debug('Close:', e.reason);\n      },\n    },\n    config: config,\n  });\n  const inputTurns = 'Compute the largest prime palindrome under 100000.';\n  session.sendClientContent({ turns: inputTurns });\n  const turns = await handleTurn();\n  for (const turn of turns) {\n    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {\n      for (const part of turn.serverContent.modelTurn.parts) {\n        if (part.text) {\n          console.debug('Received text: %s\\n', part.text);\n        }\n        else if (part.executableCode) {\n          console.debug('executableCode: %s\\n', part.executableCode.code);\n        }\n        else if (part.codeExecutionResult) {\n          console.debug('codeExecutionResult: %s\\n', part.codeExecutionResult.output);\n        }\n      }\n    }\n  }\n  session.close();\n}\nasync function main() {\n  await live().catch((e) => console.error('got error', e));\n}\nmain();\n```\n## Grounding with Google Search\nYou can enable Grounding with Google Search as part of the session configuration. This increases the Live API's accuracy and prevents hallucinations. See the [Grounding tutorial](https://ai.google.dev/gemini-api/docs/grounding) to learn more.\n### Python\n```\nimport asyncio\nfrom google import genai\nfrom google.genai import types\nclient = genai.Client(api_key=\"GEMINI_API_KEY\")\nmodel = \"gemini-live-2.5-flash-preview\"\ntools = [{'google_search': {}}]\nconfig = {\"response_modalities\": [\"TEXT\"], \"tools\": tools}\nasync def main():\n    async with client.aio.live.connect(model=model, config=config) as session:\n        prompt = \"When did the last Brazil vs. Argentina soccer match happen?\"\n        await session.send_client_content(turns={\"parts\": [{\"text\": prompt}]})\n        async for chunk in session.receive():\n            if chunk.server_content:\n                if chunk.text is not None:\n                    print(chunk.text)\n                # The model might generate and execute Python code to use Search\n                model_turn = chunk.server_content.model_turn\n                if model_turn:\n                    for part in model_turn.parts:\n                      if part.executable_code is not None:\n                        print(part.executable_code.code)\n                      if part.code_execution_result is not None:\n                        print(part.code_execution_result.output)\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n### JavaScript\n```\nimport { GoogleGenAI, Modality } from '@google/genai';\nconst ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\nconst model = 'gemini-live-2.5-flash-preview';\nconst tools = [{googleSearch: {}}]\nconst config = {\n  responseModalities: [Modality.TEXT],\n  tools: tools\n}\nasync function live() {\n  const responseQueue = [];\n  async function waitMessage() {\n    let done = false;\n    let message = undefined;\n    while (!done) {\n      message = responseQueue.shift();\n      if (message) {\n        done = true;\n      } else {\n        await new Promise((resolve) => setTimeout(resolve, 100));\n      }\n    }\n    return message;\n  }\n  async function handleTurn() {\n    const turns = [];\n    let done = false;\n    while (!done) {\n      const message = await waitMessage();\n      turns.push(message);\n      if (message.serverContent && message.serverContent.turnComplete) {\n        done = true;\n      } else if (message.toolCall) {\n        done = true;\n      }\n    }\n    return turns;\n  }\n  const session = await ai.live.connect({\n    model: model,\n    callbacks: {\n      onopen: function () {\n        console.debug('Opened');\n      },\n      onmessage: function (message) {\n        responseQueue.push(message);\n      },\n      onerror: function (e) {\n        console.debug('Error:', e.message);\n      },\n      onclose: function (e) {\n        console.debug('Close:', e.reason);\n      },\n    },\n    config: config,\n  });\n  const inputTurns = 'When did the last Brazil vs. Argentina soccer match happen?';\n  session.sendClientContent({ turns: inputTurns });\n  const turns = await handleTurn();\n  for (const turn of turns) {\n    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {\n      for (const part of turn.serverContent.modelTurn.parts) {\n        if (part.text) {\n          console.debug('Received text: %s\\n', part.text);\n        }\n        else if (part.executableCode) {\n          console.debug('executableCode: %s\\n', part.executableCode.code);\n        }\n        else if (part.codeExecutionResult) {\n          console.debug('codeExecutionResult: %s\\n', part.codeExecutionResult.output);\n        }\n      }\n    }\n  }\n  session.close();\n}\nasync function main() {\n  await live().catch((e) => console.error('got error', e));\n}\nmain();\n```\n## Combining multiple tools\nYou can combine multiple tools within the Live API, increasing your application's capabilities even more:\n### Python\n```\nprompt = \"\"\"\nHey, I need you to do three things for me.\n1. Compute the largest prime palindrome under 100000.\n2. Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024?\n3. Turn on the lights\nThanks!\n\"\"\"\ntools = [\n    {\"google_search\": {}},\n    {\"code_execution\": {}},\n    {\"function_declarations\": [turn_on_the_lights, turn_off_the_lights]},\n]\nconfig = {\"response_modalities\": [\"TEXT\"], \"tools\": tools}\n# ... remaining model call\n```\n### JavaScript\n```\nconst prompt = `Hey, I need you to do three things for me.\n1. Compute the largest prime palindrome under 100000.\n2. Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024?\n3. Turn on the lights\nThanks!\n`\nconst tools = [\n  { googleSearch: {} },\n  { codeExecution: {} },\n  { functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }\n]\nconst config = {\n  responseModalities: [Modality.TEXT],\n  tools: tools\n}\n// ... remaining model call\n```\n## What's next\n-   Check out more examples of using tools with the Live API in the [Tool use cookbook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).\n-   Get the full story on features and configurations from the [Live API Capabilities guide](https://ai.google.dev/gemini-api/docs/live-guide).\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-06-23 UTC.\n-   [Terms](//policies.google.com/terms)\n-   [Privacy](//policies.google.com/privacy)\n-   [Manage cookies](#)\n-   English\n-   Deutsch\n-   Español – América Latina\n-   Français\n-   Indonesia\n-   Italiano\n-   Polski\n-   Português – Brasil\n-   Shqip\n-   Tiếng Việt\n-   Türkçe\n-   Русский\n-   עברית\n-   العربيّة\n-   فارسی\n-   हिंदी\n-   বাংলা\n-   ภาษาไทย\n-   中文 – 简体\n-   中文 – 繁體\n-   日本語\n-   한국어",
  "timestamp": 1750765343399,
  "title": "Tool use with Live API  |  Gemini API  |  Google AI for Developers"
}