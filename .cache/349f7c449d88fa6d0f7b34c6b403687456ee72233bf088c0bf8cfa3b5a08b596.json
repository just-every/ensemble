{
  "url": "https://ai.google.dev/gemini-api/docs/audio",
  "markdown": "# Audio understanding  |  Gemini API  |  Google AI for Developers\n[Skip to main content](#main-content)\n [![Google AI for Developers](https://www.gstatic.com/devrel-devsite/prod/va55008f56463f12ba1a0c4ec3fdc81dac4d4d331f95ef7b209d2570e7d9e879b/googledevai/images/lockup-new.svg)](https://ai.google.dev/)\n[Models](https://ai.google.dev/gemini-api/docs)\n-   Gemini\n-   [\n    About\n    ](https://deepmind.google/gemini)\n-   [\n    Docs\n    ](https://ai.google.dev/gemini-api/docs)\n-   [\n    API reference\n    ](https://ai.google.dev/api)\n-   [\n    Pricing\n    ](https://ai.google.dev/pricing)\n-   Imagen\n-   [\n    About\n    ](https://deepmind.google/technologies/imagen-3/)\n-   [\n    Docs\n    ](https://ai.google.dev/gemini-api/docs/image-generation#imagen)\n-   [\n    Pricing\n    ](https://ai.google.dev/pricing)\n-   Veo\n-   [\n    About\n    ](https://deepmind.google/technologies/veo/veo-2/)\n-   [\n    Docs\n    ](https://ai.google.dev/gemini-api/docs/video)\n-   [\n    Pricing\n    ](https://ai.google.dev/pricing)\n-   Gemma\n-   [\n    About\n    ](https://deepmind.google/models/gemma)\n-   [\n    Docs\n    ](https://ai.google.dev/gemma/docs)\n-   [\n    Gemmaverse\n    ](https://ai.google.dev/gemma/gemmaverse)\nSolutions\n-   Build with Gemini\n-   [\n    Gemini API\n    ](https://ai.google.dev/gemini-api/docs)\n-   [\n    Google AI Studio\n    ](https://aistudio.google.com)\n-   Customize Gemma open models\n-   [\n    Gemma open models\n    ](https://ai.google.dev/gemma)\n-   [\n    Multi-framework with Keras\n    ](https://keras.io/keras_3/)\n-   [\n    Fine-tune in Colab\n    ](https://colab.sandbox.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb)\n-   Run on-device\n-   [\n    Google AI Edge\n    ](https://ai.google.dev/edge)\n-   [\n    Gemini Nano on Android\n    ](https://developer.android.com/ai/gemini-nano)\n-   [\n    Chrome built-in web APIs\n    ](https://developer.chrome.com/docs/ai/built-in)\n-   Build responsibly\n-   [\n    Responsible GenAI Toolkit\n    ](https://ai.google.dev/responsible)\n-   [\n    Secure AI Framework\n    ](https://saif.google)\nCode assistance\n-   [\n    Android Studio\n    ](https://developer.android.com/gemini-in-android)\n-   [\n    Chrome DevTools\n    ](https://developer.chrome.com/docs/devtools/console/understand-messages)\n-   [\n    Colab\n    ](https://colab.google)\n-   [\n    Firebase\n    ](https://firebase.google.com/products/generative-ai)\n-   [\n    Google Cloud\n    ](https://cloud.google.com/products/gemini/code-assist)\n-   [\n    JetBrains\n    ](https://plugins.jetbrains.com/plugin/8079-google-cloud-code)\n-   [\n    Jules\n    ](https://labs.google.com/jules/home)\n-   [\n    VS Code\n    ](https://marketplace.visualstudio.com/items?itemName=GoogleCloudTools.cloudcode)\nShowcase\n-   [\n    Gemini Showcase\n    ](https://ai.google.dev/showcase)\n-   [\n    Gemini API Developer Competition\n    ](https://ai.google.dev/competition)\nCommunity\n-   [\n    Google AI Forum\n    ](https://discuss.ai.google.dev)\n-   [\n    Gemini for Research\n    ](https://ai.google.dev/gemini-api/docs/gemini-for-research)\n/\n-   English\n-   Deutsch\n-   Español – América Latina\n-   Français\n-   Indonesia\n-   Italiano\n-   Polski\n-   Português – Brasil\n-   Shqip\n-   Tiếng Việt\n-   Türkçe\n-   Русский\n-   עברית\n-   العربيّة\n-   فارسی\n-   हिंदी\n-   বাংলা\n-   ภาษาไทย\n-   中文 – 简体\n-   中文 – 繁體\n-   日本語\n-   한국어\nSign in\n[Gemini API docs](https://ai.google.dev/gemini-api/docs) [API Reference](https://ai.google.dev/api) [Cookbook](https://github.com/google-gemini/cookbook) [Community](https://discuss.ai.google.dev/c/gemini-api/) [![Google AI for Developers](https://www.gstatic.com/devrel-devsite/prod/va55008f56463f12ba1a0c4ec3fdc81dac4d4d331f95ef7b209d2570e7d9e879b/googledevai/images/lockup-new.svg)](https://ai.google.dev/)\n-   [Models](https://ai.google.dev/gemini-api/docs)\n    -   More\n    -   [Gemini API docs](https://ai.google.dev/gemini-api/docs)\n    -   [API Reference](https://ai.google.dev/api)\n    -   [Cookbook](https://github.com/google-gemini/cookbook)\n    -   [Community](https://discuss.ai.google.dev/c/gemini-api/)\n-   Solutions\n    -   More\n-   Code assistance\n    -   More\n-   Showcase\n    -   More\n-   Community\n    -   More\n-   Get started\n-   [Overview](https://ai.google.dev/gemini-api/docs)\n-   [Quickstart](https://ai.google.dev/gemini-api/docs/quickstart)\n-   [API keys](https://ai.google.dev/gemini-api/docs/api-key)\n-   [Libraries](https://ai.google.dev/gemini-api/docs/libraries)\n-   [OpenAI compatibility](https://ai.google.dev/gemini-api/docs/openai)\n-   Models\n-   [All models](https://ai.google.dev/gemini-api/docs/models)\n-   [Pricing](https://ai.google.dev/gemini-api/docs/pricing)\n-   [Rate limits](https://ai.google.dev/gemini-api/docs/rate-limits)\n-   [Billing info](https://ai.google.dev/gemini-api/docs/billing)\n-   Model Capabilities\n-   [Text generation](https://ai.google.dev/gemini-api/docs/text-generation)\n-   [Image generation](https://ai.google.dev/gemini-api/docs/image-generation)\n-   [Video generation](https://ai.google.dev/gemini-api/docs/video)\n-   [Speech generation](https://ai.google.dev/gemini-api/docs/speech-generation)\n-   [Music generation](https://ai.google.dev/gemini-api/docs/music-generation)\n-   [Long context](https://ai.google.dev/gemini-api/docs/long-context)\n-   [Structured output](https://ai.google.dev/gemini-api/docs/structured-output)\n-   [Thinking](https://ai.google.dev/gemini-api/docs/thinking)\n-   [Function calling](https://ai.google.dev/gemini-api/docs/function-calling)\n-   [Document understanding](https://ai.google.dev/gemini-api/docs/document-processing)\n-   [Image understanding](https://ai.google.dev/gemini-api/docs/image-understanding)\n-   [Video understanding](https://ai.google.dev/gemini-api/docs/video-understanding)\n-   [Audio understanding](https://ai.google.dev/gemini-api/docs/audio)\n-   [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)\n-   [URL context](https://ai.google.dev/gemini-api/docs/url-context)\n-   [Google Search](https://ai.google.dev/gemini-api/docs/google-search)\n-   Guides\n-   [Prompt engineering](https://ai.google.dev/gemini-api/docs/prompting-strategies)\n-   Live API\n    -   [Get started](https://ai.google.dev/gemini-api/docs/live)\n    -   [Capabilities](https://ai.google.dev/gemini-api/docs/live-guide)\n    -   [Tool use](https://ai.google.dev/gemini-api/docs/live-tools)\n    -   [Session management](https://ai.google.dev/gemini-api/docs/live-session)\n    -   [Ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens)\n-   [Context caching](https://ai.google.dev/gemini-api/docs/caching)\n-   [Files API](https://ai.google.dev/gemini-api/docs/files)\n-   [Token counting](https://ai.google.dev/gemini-api/docs/tokens)\n-   Fine-tuning\n    -   [Intro to fine-tuning](https://ai.google.dev/gemini-api/docs/model-tuning)\n    -   [Fine-tuning tutorial](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial)\n-   [Embeddings](https://ai.google.dev/gemini-api/docs/embeddings)\n-   Safety\n    -   [Safety settings](https://ai.google.dev/gemini-api/docs/safety-settings)\n    -   [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance)\n-   Resources\n-   [Migrate to Gen AI SDK](https://ai.google.dev/gemini-api/docs/migrate)\n-   [Release notes](https://ai.google.dev/gemini-api/docs/changelog)\n-   [API troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshooting)\n-   Open-Source Frameworks\n    -   [LangChain & LangGraph](https://ai.google.dev/gemini-api/docs/langgraph-example)\n    -   [CrewAI](https://ai.google.dev/gemini-api/docs/crewai-example)\n-   AI Studio\n    -   [Google AI Studio quickstart](https://ai.google.dev/gemini-api/docs/ai-studio-quickstart)\n    -   [LearnLM](https://ai.google.dev/gemini-api/docs/learnlm)\n    -   [AI Studio troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio)\n    -   [Google Workspace](https://ai.google.dev/gemini-api/docs/workspace)\n-   Google Cloud Platform\n    -   [VertexAI Gemini API](https://ai.google.dev/gemini-api/docs/migrate-to-cloud)\n    -   [OAuth authentication](https://ai.google.dev/gemini-api/docs/oauth)\n-   Policies\n-   [Terms of service](https://ai.google.dev/gemini-api/terms)\n-   [Available regions](https://ai.google.dev/gemini-api/docs/available-regions)\n-   [Additional usage polices](https://ai.google.dev/gemini-api/docs/usage-policies)\n-   Gemini\n-   [About](https://deepmind.google/gemini)\n-   [Docs](https://ai.google.dev/gemini-api/docs)\n-   [API reference](https://ai.google.dev/api)\n-   [Pricing](https://ai.google.dev/pricing)\n-   Imagen\n-   [About](https://deepmind.google/technologies/imagen-3/)\n-   [Docs](https://ai.google.dev/gemini-api/docs/image-generation#imagen)\n-   [Pricing](https://ai.google.dev/pricing)\n-   Veo\n-   [About](https://deepmind.google/technologies/veo/veo-2/)\n-   [Docs](https://ai.google.dev/gemini-api/docs/video)\n-   [Pricing](https://ai.google.dev/pricing)\n-   Gemma\n-   [About](https://deepmind.google/models/gemma)\n-   [Docs](https://ai.google.dev/gemma/docs)\n-   [Gemmaverse](https://ai.google.dev/gemma/gemmaverse)\n-   Build with Gemini\n-   [Gemini API](https://ai.google.dev/gemini-api/docs)\n-   [Google AI Studio](https://aistudio.google.com)\n-   Customize Gemma open models\n-   [Gemma open models](https://ai.google.dev/gemma)\n-   [Multi-framework with Keras](https://keras.io/keras_3/)\n-   [Fine-tune in Colab](https://colab.sandbox.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb)\n-   Run on-device\n-   [Google AI Edge](https://ai.google.dev/edge)\n-   [Gemini Nano on Android](https://developer.android.com/ai/gemini-nano)\n-   [Chrome built-in web APIs](https://developer.chrome.com/docs/ai/built-in)\n-   Build responsibly\n-   [Responsible GenAI Toolkit](https://ai.google.dev/responsible)\n-   [Secure AI Framework](https://saif.google)\n-   [Android Studio](https://developer.android.com/gemini-in-android)\n-   [Chrome DevTools](https://developer.chrome.com/docs/devtools/console/understand-messages)\n-   [Colab](https://colab.google)\n-   [Firebase](https://firebase.google.com/products/generative-ai)\n-   [Google Cloud](https://cloud.google.com/products/gemini/code-assist)\n-   [JetBrains](https://plugins.jetbrains.com/plugin/8079-google-cloud-code)\n-   [Jules](https://labs.google.com/jules/home)\n-   [VS Code](https://marketplace.visualstudio.com/items?itemName=GoogleCloudTools.cloudcode)\n-   [Gemini Showcase](https://ai.google.dev/showcase)\n-   [Gemini API Developer Competition](https://ai.google.dev/competition)\n-   [Google AI Forum](https://discuss.ai.google.dev)\n-   [Gemini for Research](https://ai.google.dev/gemini-api/docs/gemini-for-research)\nIntroducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)\n-   [Home](https://ai.google.dev/)\n-   [Gemini API](https://ai.google.dev/gemini-api)\n-   [Models](https://ai.google.dev/gemini-api/docs)\nSend feedback\n# Audio understanding\nGemini can analyze and understand audio input, enabling use cases like the following:\n-   Describe, summarize, or answer questions about audio content.\n-   Provide a transcription of the audio.\n-   Analyze specific segments of the audio.\nThis guide shows you how to use the Gemini API to generate a text response to audio input.\n### Before you begin\nBefore calling the Gemini API, ensure you have [your SDK of choice](https://ai.google.dev/gemini-api/docs/downloads) installed, and a [Gemini API key](https://ai.google.dev/gemini-api/docs/api-key) configured and ready to use.\n## Input audio\nYou can provide audio data to Gemini in the following ways:\n-   [Upload an audio file](#upload-audio) before making a request to `generateContent`.\n-   [Pass inline audio data](#inline-audio) with the request to `generateContent`.\n### Upload an audio file\nYou can use the [Files API](https://ai.google.dev/gemini-api/docs/files) to upload an audio file. Always use the Files API when the total request size (including the files, text prompt, system instructions, etc.) is larger than 20 MB.\nThe following code uploads an audio file and then uses the file in a call to `generateContent`.\n### Python\n```\nfrom google import genai\nclient = genai.Client(api_key=\"GOOGLE_API_KEY\")\nmyfile = client.files.upload(file=\"path/to/sample.mp3\")\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\", contents=[\"Describe this audio clip\", myfile]\n)\nprint(response.text)\n```\n### JavaScript\n```\nimport {\n  GoogleGenAI,\n  createUserContent,\n  createPartFromUri,\n} from \"@google/genai\";\nconst ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\nasync function main() {\n  const myfile = await ai.files.upload({\n    file: \"path/to/sample.mp3\",\n    config: { mimeType: \"audio/mp3\" },\n  });\n  const response = await ai.models.generateContent({\n    model: \"gemini-2.5-flash\",\n    contents: createUserContent([\n      createPartFromUri(myfile.uri, myfile.mimeType),\n      \"Describe this audio clip\",\n    ]),\n  });\n  console.log(response.text);\n}\nawait main();\n```\n### Go\n```\npackage main\nimport (\n  \"context\"\n  \"fmt\"\n  \"os\"\n  \"google.golang.org/genai\"\n)\nfunc main() {\n  ctx := context.Background()\n  client, _ := genai.NewClient(ctx, &genai.ClientConfig{\n      APIKey:  os.Getenv(\"GEMINI_API_KEY\"),\n      Backend: genai.BackendGeminiAPI,\n  })\n  localAudioPath := \"/path/to/sample.mp3\"\n  uploadedFile, _ := client.Files.UploadFromPath(\n      ctx,\n      localAudioPath,\n      nil,\n  )\n  parts := []*genai.Part{\n      genai.NewPartFromText(\"Describe this audio clip\"),\n      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),\n  }\n  contents := []*genai.Content{\n      genai.NewContentFromParts(parts, genai.RoleUser),\n  }\n  result, _ := client.Models.GenerateContent(\n      ctx,\n      \"gemini-2.5-flash\",\n      contents,\n      nil,\n  )\n  fmt.Println(result.Text())\n}\n```\n### REST\n```\nAUDIO_PATH=\"path/to/sample.mp3\"\nMIME_TYPE=$(file -b --mime-type \"${AUDIO_PATH}\")\nNUM_BYTES=$(wc -c < \"${AUDIO_PATH}\")\nDISPLAY_NAME=AUDIO\ntmp_header_file=upload-header.tmp\n# Initial resumable request defining metadata.\n# The upload url is in the response headers dump them to a file.\ncurl \"https://generativelanguage.googleapis.com/upload/v1beta/files?key=${GOOGLE_API_KEY}\" \\\n  -D upload-header.tmp \\\n  -H \"X-Goog-Upload-Protocol: resumable\" \\\n  -H \"X-Goog-Upload-Command: start\" \\\n  -H \"X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}\" \\\n  -H \"X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{'file': {'display_name': '${DISPLAY_NAME}'}}\" 2> /dev/null\nupload_url=$(grep -i \"x-goog-upload-url: \" \"${tmp_header_file}\" | cut -d\" \" -f2 | tr -d \"\\r\")\nrm \"${tmp_header_file}\"\n# Upload the actual bytes.\ncurl \"${upload_url}\" \\\n  -H \"Content-Length: ${NUM_BYTES}\" \\\n  -H \"X-Goog-Upload-Offset: 0\" \\\n  -H \"X-Goog-Upload-Command: upload, finalize\" \\\n  --data-binary \"@${AUDIO_PATH}\" 2> /dev/null > file_info.json\nfile_uri=$(jq \".file.uri\" file_info.json)\necho file_uri=$file_uri\n# Now generate content using that file\ncurl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=$GOOGLE_API_KEY\" \\\n    -H 'Content-Type: application/json' \\\n    -X POST \\\n    -d '{\n      \"contents\": [{\n        \"parts\":[\n          {\"text\": \"Describe this audio clip\"},\n          {\"file_data\":{\"mime_type\": \"${MIME_TYPE}\", \"file_uri\": '$file_uri'}}]\n        }]\n      }' 2> /dev/null > response.json\ncat response.json\necho\njq \".candidates[].content.parts[].text\" response.json\n```\nTo learn more about working with media files, see [Files API](https://ai.google.dev/gemini-api/docs/files).\n### Pass audio data inline\nInstead of uploading an audio file, you can pass inline audio data in the request to `generateContent`:\n### Python\n```\nfrom google.genai import types\nwith open('path/to/small-sample.mp3', 'rb') as f:\n    audio_bytes = f.read()\nresponse = client.models.generate_content(\n  model='gemini-2.5-flash',\n  contents=[\n    'Describe this audio clip',\n    types.Part.from_bytes(\n      data=audio_bytes,\n      mime_type='audio/mp3',\n    )\n  ]\n)\nprint(response.text)\n```\n### JavaScript\n```\nimport { GoogleGenAI } from \"@google/genai\";\nimport * as fs from \"node:fs\";\nconst ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\nconst base64AudioFile = fs.readFileSync(\"path/to/small-sample.mp3\", {\n  encoding: \"base64\",\n});\nconst contents = [\n  { text: \"Please summarize the audio.\" },\n  {\n    inlineData: {\n      mimeType: \"audio/mp3\",\n      data: base64AudioFile,\n    },\n  },\n];\nconst response = await ai.models.generateContent({\n  model: \"gemini-2.5-flash\",\n  contents: contents,\n});\nconsole.log(response.text);\n```\n### Go\n```\npackage main\nimport (\n  \"context\"\n  \"fmt\"\n  \"os\"\n  \"google.golang.org/genai\"\n)\nfunc main() {\n  ctx := context.Background()\n  client, _ := genai.NewClient(ctx, &genai.ClientConfig{\n      APIKey:  os.Getenv(\"GEMINI_API_KEY\"),\n      Backend: genai.BackendGeminiAPI,\n  })\n  audioBytes, _ := os.ReadFile(\"/path/to/small-sample.mp3\")\n  parts := []*genai.Part{\n      genai.NewPartFromText(\"Describe this audio clip\"),\n    &genai.Part{\n      InlineData: &genai.Blob{\n        MIMEType: \"audio/mp3\",\n        Data:     audioBytes,\n      },\n    },\n  }\n  contents := []*genai.Content{\n      genai.NewContentFromParts(parts, genai.RoleUser),\n  }\n  result, _ := client.Models.GenerateContent(\n      ctx,\n      \"gemini-2.5-flash\",\n      contents,\n      nil,\n  )\n  fmt.Println(result.Text())\n}\n```\nA few things to keep in mind about inline audio data:\n-   The maximum request size is 20 MB, which includes text prompts, system instructions, and files provided inline. If your file's size will make the _total request size_ exceed 20 MB, then use the Files API to [upload an audio file](#upload-audio) for use in the request.\n-   If you're using an audio sample multiple times, it's more efficient to [upload an audio file](#upload-audio).\n## Get a transcript\nTo get a transcript of audio data, just ask for it in the prompt:\n### Python\n```\nmyfile = client.files.upload(file='path/to/sample.mp3')\nprompt = 'Generate a transcript of the speech.'\nresponse = client.models.generate_content(\n  model='gemini-2.5-flash',\n  contents=[prompt, myfile]\n)\nprint(response.text)\n```\n### JavaScript\n```\nimport {\n  GoogleGenAI,\n  createUserContent,\n  createPartFromUri,\n} from \"@google/genai\";\nconst ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\nconst myfile = await ai.files.upload({\n  file: \"path/to/sample.mp3\",\n  config: { mimeType: \"audio/mpeg\" },\n});\nconst result = await ai.models.generateContent({\n  model: \"gemini-2.5-flash\",\n  contents: createUserContent([\n    createPartFromUri(myfile.uri, myfile.mimeType),\n    \"Generate a transcript of the speech.\",\n  ]),\n});\nconsole.log(\"result.text=\", result.text);\n```\n### Go\n```\npackage main\nimport (\n  \"context\"\n  \"fmt\"\n  \"os\"\n  \"google.golang.org/genai\"\n)\nfunc main() {\n  ctx := context.Background()\n  client, _ := genai.NewClient(ctx, &genai.ClientConfig{\n      APIKey:  os.Getenv(\"GEMINI_API_KEY\"),\n      Backend: genai.BackendGeminiAPI,\n  })\n  localAudioPath := \"/path/to/sample.mp3\"\n  uploadedFile, _ := client.Files.UploadFromPath(\n      ctx,\n      localAudioPath,\n      nil,\n  )\n  parts := []*genai.Part{\n      genai.NewPartFromText(\"Generate a transcript of the speech.\"),\n      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),\n  }\n  contents := []*genai.Content{\n      genai.NewContentFromParts(parts, genai.RoleUser),\n  }\n  result, _ := client.Models.GenerateContent(\n      ctx,\n      \"gemini-2.5-flash\",\n      contents,\n      nil,\n  )\n  fmt.Println(result.Text())\n}\n```\n## Refer to timestamps\nYou can refer to specific sections of an audio file using timestamps of the form `MM:SS`. For example, the following prompt requests a transcript that\n-   Starts at 2 minutes 30 seconds from the beginning of the file.\n-   Ends at 3 minutes 29 seconds from the beginning of the file.\n### Python\n```\n# Create a prompt containing timestamps.\nprompt = \"Provide a transcript of the speech from 02:30 to 03:29.\"\n```\n### JavaScript\n```\n// Create a prompt containing timestamps.\nconst prompt = \"Provide a transcript of the speech from 02:30 to 03:29.\"\n```\n### Go\n```\npackage main\nimport (\n  \"context\"\n  \"fmt\"\n  \"os\"\n  \"google.golang.org/genai\"\n)\nfunc main() {\n  ctx := context.Background()\n  client, _ := genai.NewClient(ctx, &genai.ClientConfig{\n      APIKey:  os.Getenv(\"GEMINI_API_KEY\"),\n      Backend: genai.BackendGeminiAPI,\n  })\n  localAudioPath := \"/path/to/sample.mp3\"\n  uploadedFile, _ := client.Files.UploadFromPath(\n      ctx,\n      localAudioPath,\n      nil,\n  )\n  parts := []*genai.Part{\n      genai.NewPartFromText(\"Provide a transcript of the speech \" +\n                            \"between the timestamps 02:30 and 03:29.\"),\n      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),\n  }\n  contents := []*genai.Content{\n      genai.NewContentFromParts(parts, genai.RoleUser),\n  }\n  result, _ := client.Models.GenerateContent(\n      ctx,\n      \"gemini-2.5-flash\",\n      contents,\n      nil,\n  )\n  fmt.Println(result.Text())\n}\n```\n## Count tokens\nCall the `countTokens` method to get a count of the number of tokens in an audio file. For example:\n### Python\n```\nresponse = client.models.count_tokens(\n  model='gemini-2.5-flash',\n  contents=[myfile]\n)\nprint(response)\n```\n### JavaScript\n```\nimport {\n  GoogleGenAI,\n  createUserContent,\n  createPartFromUri,\n} from \"@google/genai\";\nconst ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\nconst myfile = await ai.files.upload({\n  file: \"path/to/sample.mp3\",\n  config: { mimeType: \"audio/mpeg\" },\n});\nconst countTokensResponse = await ai.models.countTokens({\n  model: \"gemini-2.5-flash\",\n  contents: createUserContent([\n    createPartFromUri(myfile.uri, myfile.mimeType),\n  ]),\n});\nconsole.log(countTokensResponse.totalTokens);\n```\n### Go\n```\npackage main\nimport (\n  \"context\"\n  \"fmt\"\n  \"os\"\n  \"google.golang.org/genai\"\n)\nfunc main() {\n  ctx := context.Background()\n  client, _ := genai.NewClient(ctx, &genai.ClientConfig{\n      APIKey:  os.Getenv(\"GEMINI_API_KEY\"),\n      Backend: genai.BackendGeminiAPI,\n  })\n  localAudioPath := \"/path/to/sample.mp3\"\n  uploadedFile, _ := client.Files.UploadFromPath(\n      ctx,\n      localAudioPath,\n      nil,\n  )\n  parts := []*genai.Part{\n      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),\n  }\n  contents := []*genai.Content{\n      genai.NewContentFromParts(parts, genai.RoleUser),\n  }\n  tokens, _ := client.Models.CountTokens(\n      ctx,\n      \"gemini-2.5-flash\",\n      contents,\n      nil,\n  )\n  fmt.Printf(\"File %s is %d tokens\\n\", localAudioPath, tokens.TotalTokens)\n}\n```\n## Supported audio formats\nGemini supports the following audio format MIME types:\n-   WAV - `audio/wav`\n-   MP3 - `audio/mp3`\n-   AIFF - `audio/aiff`\n-   AAC - `audio/aac`\n-   OGG Vorbis - `audio/ogg`\n-   FLAC - `audio/flac`\n## Technical details about audio\n-   Gemini represents each second of audio as 32 tokens; for example, one minute of audio is represented as 1,920 tokens.\n-   Gemini can \"understand\" non-speech components, such as birdsong or sirens.\n-   The maximum supported length of audio data in a single prompt is 9.5 hours. Gemini doesn't limit the _number_ of audio files in a single prompt; however, the total combined length of all audio files in a single prompt can't exceed 9.5 hours.\n-   Gemini downsamples audio files to a 16 Kbps data resolution.\n-   If the audio source contains multiple channels, Gemini combines those channels into a single channel.\n## What's next\nThis guide shows how to generate text in response to audio data. To learn more, see the following resources:\n-   [File prompting strategies](https://ai.google.dev/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.\n-   [System instructions](https://ai.google.dev/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.\n-   [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-06-17 UTC.\n-   [Terms](//policies.google.com/terms)\n-   [Privacy](//policies.google.com/privacy)\n-   [Manage cookies](#)\n-   English\n-   Deutsch\n-   Español – América Latina\n-   Français\n-   Indonesia\n-   Italiano\n-   Polski\n-   Português – Brasil\n-   Shqip\n-   Tiếng Việt\n-   Türkçe\n-   Русский\n-   עברית\n-   العربيّة\n-   فارسی\n-   हिंदी\n-   বাংলা\n-   ภาษาไทย\n-   中文 – 简体\n-   中文 – 繁體\n-   日本語\n-   한국어",
  "timestamp": 1750652733126,
  "title": "Audio understanding  |  Gemini API  |  Google AI for Developers"
}