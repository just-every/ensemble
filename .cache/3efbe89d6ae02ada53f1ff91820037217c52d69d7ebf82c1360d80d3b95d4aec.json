{
  "url": "https://elevenlabs.io/docs/api-reference/speech-to-text/convert",
  "markdown": "# Create transcript | ElevenLabs Documentation\nTranscribe an audio or video file. If webhook is set to true, the request will be processed asynchronously and results sent to configured webhooks.\n### Headers\nxi-api-keystringRequired\n### Query parameters\nenable\\_loggingbooleanOptionalDefaults to `true`\nWhen enable\\_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.\n### Request\nThis endpoint expects a multipart form containing an optional file.\nmodel\\_idstringRequired\nThe ID of the model to use for transcription, currently only ‘scribe\\_v1’ and ‘scribe\\_v1\\_experimental’ are available.\nfilefileOptional\nThe file to transcribe. All major audio and video formats are supported. Exactly one of the file or cloud\\_storage\\_url parameters must be provided. The file size must be less than 1GB.\nlanguage\\_codestring or nullOptional\nAn ISO-639-1 or ISO-639-3 language\\_code corresponding to the language of the audio file. Can sometimes improve transcription performance if known beforehand. Defaults to null, in this case the language is predicted automatically.\ntag\\_audio\\_eventsbooleanOptionalDefaults to `true`\nWhether to tag audio events like (laughter), (footsteps), etc. in the transcription.\nnum\\_speakersinteger or nullOptional`>=1``<=32`\nThe maximum amount of speakers talking in the uploaded file. Can help with predicting who speaks when. The maximum amount of speakers that can be predicted is 32. Defaults to null, in this case the amount of speakers is set to the maximum value the model supports.\ntimestamps\\_granularityenumOptionalDefaults to `word`\nThe granularity of the timestamps in the transcription. ‘word’ provides word-level timestamps and ‘character’ provides character-level timestamps per word.\nAllowed values:\ndiarizebooleanOptionalDefaults to `false`\nWhether to annotate which speaker is currently talking in the uploaded file.\nadditional\\_formatslist of objectsOptional\nA list of additional formats to export the transcript to.\nfile\\_formatenumOptionalDefaults to `other`\nThe format of input audio. Options are ‘pcm\\_s16le\\_16’ or ‘other’ For `pcm_s16le_16`, the input audio must be 16-bit PCM at a 16kHz sample rate, single channel (mono), and little-endian byte order. Latency will be lower than with passing an encoded waveform.\nAllowed values:\ncloud\\_storage\\_urlstring or nullOptional\nThe valid AWS S3, Cloudflare R2 or Google Cloud Storage URL of the file to transcribe. Exactly one of the file or cloud\\_storage\\_url parameters must be provided. The file must be a valid publicly accessible cloud storage URL. The file size must be less than 2GB. URL can be pre-signed.\nwebhookbooleanOptionalDefaults to `false`\nWhether to send the transcription result to configured speech-to-text webhooks. If set the request will return early without the transcription, which will be delivered later via webhook.\ntemperaturedouble or nullOptional`>=0``<=2`\nControls the randomness of the transcription output. Accepts values between 0.0 and 2.0, where higher values result in more diverse and less deterministic results. If omitted, we will use a temperature based on the model you selected which is usually 0.\n### Response\nSynchronous transcription result\nlanguage\\_codestring\nThe detected language code (e.g. ‘eng’ for English).\nlanguage\\_probabilitydouble\nThe confidence score of the language detection (0 to 1).\ntextstring\nThe raw text of the transcription.\nwordslist of objects\nList of words with their timing information.\nadditional\\_formatslist of nullable objects or null\nRequested additional formats of the transcript.\n### Errors",
  "timestamp": 1750637738579,
  "title": "Create transcript | ElevenLabs Documentation"
}